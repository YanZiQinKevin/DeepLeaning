Supervised learning
	prediction
	Classification(discrete labels), Regression(real value)
Unsupervised learning
	Clustering
	probability
	Finding association
	Dimension reduction

Linear classifier
	Perceptron
	Logistic regression
	Support vector machine




Linear regression: Weight Features(known) Target 
	Target = Weight * Features
	cost= square(prediction-real)

Logistic Regression: 逻辑回归， 用于binary的判断
			h(theta) = 1/(1+exp(-theta * x))
			注意： logistic Regression 的cost function 也不一样

Gradient Descent: cost = square(prediction-real)	
					   = square(W * x - y)
					   = square(wx)

	结果是一个一元二次函数，函数每个点的切线称之为梯度，当梯度斜率为0时，为optimal solution
	学习速率 Leaning rate 则影响了“点”每次下降移动的距离

Activation function:
	y=AF(Wx) 
	AF: relu sigmoid, tanh  (可微分)

Overfitting:
	误差太小
	数据太少
	1,regularization:
		L1: cost= square(Wx -real y) + abs(W)
		L2: cost= square(Wx -real y) + square(W)

	2,dropout: 随机忽略一些节点
	3, batch normalization
	4, early stopping
Convolutional Neural Network:
	convolution layer(卷积层):

	Filter size and Stride , conver all original image  

	Relu layer(线性整流层)：
	f(x) =max(0,x)
	在某一点之前为0，只后为线性，用于激活函数

	pooling layer （池化层）:
	用filters 找最大值

	fully connected layer（全连接层): 
	Assume : Pooling 5 * 5 *3 我们可以转化为 1 * 75
			  Fully Connected 75 *2
			  //两矩阵相乘
			 Thus: output 1 * 2 


	得到的结果是调整滤镜矩阵的参数


Recurrent Neural Networl:
	预测顺序排列
	用于翻译、手写输入、对于图片生成一句话。
	每个参数的前后有逻辑关系
	ht=fw(ht-1, xt)
	ht= tanh(Whh ht-1 + Wxh xt)
	yt=Why ht
LSTM: long Short-Term Memory (忘记控制)

Backpropagation 算法 (BP算法)
反方向 （输出层->隐藏层->输入层）来最小化误差 error 来更新每个连接的weight
4个公式

有监督学习，无监督学习，分类，聚类，回归
有监督： 分类, 回归， 决策树
无监督： 聚类(Kmeans)

聚类：
kmeans: Iteratively reassion points
input k,data[n]:
1,选择k个初始中心点，c[0]=data[0].c[k-1]=data[k-1]
2，对于data[0]..data[n],分别与c[0],c[k-1]比较，假定与c[i]差值最少，标记为i
3, 对于所有标记为i点， 重新计算c[i]=（所有标记为i的data[i]之和）/标记为i的个数
4，重复2，3 直到c[i]值的变化小于给定阈值
缺点：初始点的选择有关，局部优化，需要k值

感知器分类算法
Classifiers: Nearest neighbor
	KNN: How large the k is
	1-NN provable has error that is at most twice Bayes optimal error
Generalization 
	Components of generalization error
Bias: how much the average model over all training sets differ from the the model 
Veriance: how much models estimated from different training sets differ from each oher
underfitting: model is too simple to represent all the relevant class characteristics
	high bias and low variance
	high training error and high test error


